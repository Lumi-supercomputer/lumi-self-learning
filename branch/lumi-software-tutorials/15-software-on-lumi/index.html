<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using software on LUMI &mdash; Introduction to LUMI</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css" />
      <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
      <link rel="stylesheet" type="text/css" href="../_static/overrides.css" />

  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/minipres.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Scheduler and batch jobs" href="../16-scheduler-and-batch-jobs/" />
    <link rel="prev" title="Software on supercomputers - Introduction to the concept of modules" href="../14-modules-and-software-stacks/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Introduction to LUMI
              <img src="../_static/LUMI_light.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../10-introduction-to-supercomputing-and-lumi/">Introduction to supercomputing and LUMI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11-prerequisites/">Prerequisites (accounts, projects and connecting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12-lumi-environment/">LUMI environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13a-exploring-remote-resources/">Exploring Remote Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13-lumi-disk-areas/">Where to store files in LUMI computing environment?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14-modules-and-software-stacks/">Software on supercomputers - Introduction to the concept of modules</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using software on LUMI</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-software-there-are-available-on-lumi">What software there are available on LUMI?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#good-to-know-software-stacks-on-lumi">Good to know: Software stacks on LUMI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#modules-on-lumi">Modules on LUMI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tutorial-load-the-nano-editor-and-write-a-simple-script">Tutorial: Load the Nano editor and write a simple script</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tutorial-install-a-software-with-easybuild">Tutorial: Install a software with EasyBuild</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#usage-of-the-software">Usage of the software</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tutorial-pytorch-with-ddp">Tutorial: PyTorch with DDP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setup">Setup</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#st-step">1st Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nd-step">2nd Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rd-step">3rd Step</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#use-it">Use it</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tutorial-tensorflow-with-horovod">Tutorial: Tensorflow with Horovod</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Setup</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">1st Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">2nd Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">3rd Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#th-step">4th Step</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Use it</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tutorial-adding-your-own-easyconfig-to-lumi">Tutorial: Adding your own EasyConfig to LUMI</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../16-scheduler-and-batch-jobs/">Scheduler and batch jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17-parallel/">Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18-using-resources-efficiently/">Using resources efficiently</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19-managing-data/">Managing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../20-responsibility/">Responsibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21-installing-own-applications/">Installing own applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22-working-with-containers/">Working with containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23-advanced-topics/">Advanced topics (e.g. high-throughput workflows, CPU-GPU binding)</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Introduction to LUMI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Using software on LUMI</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Lumi-supercomputer/lumi-self-learning/blob/main/content/15-software-on-lumi.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-software-on-lumi">
<h1>Using software on LUMI<a class="headerlink" href="#using-software-on-lumi" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p>How do I know if my software is available on LUMI?</p></li>
<li><p>How do I use the software on LUMI?</p></li>
</ul>
<p>This chapter introduces to key points of how to identify the available software on LUMI, and how to get these software in use. We also give examples with selected software how this works in practise. In the chapter (installing own applications) we give instruction how to install other software that is not available via the supported EasyBuild recipes by the LUMI user support team, or by LUMI local organizations.</p>
<p>For a more comprehensive look to modules and software on LUMI (for people who already understand something about these concepts) see the LUMI documentation of <a class="reference external" href="https://docs.lumi-supercomputer.eu/runjobs/lumi_env/Lmod_modules/">Lmod modules</a>, <a class="reference external" href="https://docs.lumi-supercomputer.eu/runjobs/lumi_env/softwarestacks/">software stacks</a> and <a class="reference external" href="https://docs.lumi-supercomputer.eu/software/">using software on LUMI</a>.</p>
<section id="what-software-there-are-available-on-lumi">
<h2>What software there are available on LUMI?<a class="headerlink" href="#what-software-there-are-available-on-lumi" title="Permalink to this heading"></a></h2>
<p>To see the available software on LUMI, one doesn’t need to login to the LUMI supercomputer. To see the software available in the central LUMI software collection, just open the <a class="reference external" href="https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/">LUMI software library</a> page, which lists all the software that are either <em>pre-installed</em> to the system, or easily <em>user installable</em>.</p>
<p>Let’s take a few examples.</p>
<p>First, let’s search Gnuplot, which is a simple graphics software, and see the page about it.
When we open the page about Gnuplot, we see that there is a green box that reads <code class="docutils literal notranslate"><span class="pre">pre-installed</span></code>. This means that the software is already available in the system, and one can directly use the typical Lmod commands (e.g. <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span></code>) with it. (We’ll see a bit later how the Lmod commands work in practise, and what the term <em>pre-installed</em> means.)</p>
<p>As a second example, let’s take a look at the page of Gromacs, a software for molecular dynamics. On this page we have a blue box that reads <code class="docutils literal notranslate"><span class="pre">user</span> <span class="pre">installable</span></code>. This means that Lmod can’t directly find the software on LUMI, but user needs to <em>install</em> it first. This installation is not a difficult process, and only requires typing few commands. We’ll come back to how to do this with specific examples later in this guide.</p>
<p>Besides the software available in the central LUMI software collection, there are software collections available by local LUMI organizations. Currently (April 2024) there is available an additional software stack by CSC, Finland. To see these available software via this collection, visit the <a class="reference external" href="https://docs.lumi-supercomputer.eu/software/local/csc/">page about local software collections</a> in the LUMI documentation.</p>
</section>
<section id="good-to-know-software-stacks-on-lumi">
<h2>Good to know: Software stacks on LUMI<a class="headerlink" href="#good-to-know-software-stacks-on-lumi" title="Permalink to this heading"></a></h2>
<p>A software stack is a predetermined set of modules, libraries and compilers, and everything that is needed to run applications on the system. On LUMI there are a couple of different software stacks available. These can be considered as different approaches with a little bit different features. Also there can be different versions of these software stacks available at the same time. It depends on your use case, and perhaps your previous experience with these approaches, which software stacks you wish to use.</p>
<p>As LUMI is a HPE Cray machine, the basis of all the software stacks and environments on LUMI is a Cray environment. This is what you get as a default, once you login to LUMI. It is recommended to load one of the available software stacks on top of this.</p>
<p>The main software stack on LUMI is the <code class="docutils literal notranslate"><span class="pre">LUMI</span> <span class="pre">software</span> <span class="pre">stack</span></code>, which is maintained and supported by the LUMI user support team. There are several versions of it available at the same time, as the components of the software stacks regularily get updates. Old software stack versions always get disabled at some point, and new ones are added.</p>
<p>In the following examples in this guide we will be using a version of the main software stack on LUMI, i.e. version of the “LUMI software stack”.</p>
<p>The figure below drafts the available software stacks, when one logins to LUMI. At first there is the Basic Cray Environment. On top of that one is recommended to load either the CrayEnv, a version of the LUMI software stacks, or a version of the Spack package manager.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Login
|
Basic Cray Environment
|
|------ CrayEnv
|      
|------ LUMI stacks (LUMI/22.12, LUMI/23.09, ...)
|      
`------ Spack (Spack/23.03, Spack/23.09, ...)

</pre></div>
</div>
<p><a class="reference external" href="https://docs.lumi-supercomputer.eu/runjobs/lumi_env/softwarestacks/#crayenv"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">Cray</span> <span class="pre">environment</span></code></a> (CrayEnv) contains some additional tools on top of the default environemnt what you get at login.</p>
<p><a class="reference external" href="https://docs.lumi-supercomputer.eu/runjobs/lumi_env/softwarestacks/#lumi"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">LUMI</span> <span class="pre">stacks</span></code></a> are the main software stacks on LUMI that is build on top of the basic Cray environment using EasyBuild.
This is what we will be using in the following examples.</p>
<p><a class="reference external" href="https://docs.lumi-supercomputer.eu/software/installing/spack/"><code class="docutils literal notranslate"><span class="pre">Spack</span></code></a> is available, but it is only recommended if you are already familiar with using Spack. It is offered “as-is”. No support with developing or debugging Spack modules is provided by the LUMI user support team.</p>
<p>This is just a very brief description. In case you would like to know more, please see the <a class="reference external" href="https://docs.lumi-supercomputer.eu/runjobs/lumi_env/softwarestacks/">LUMI documentation</a>.</p>
</section>
<section id="modules-on-lumi">
<h2>Modules on LUMI<a class="headerlink" href="#modules-on-lumi" title="Permalink to this heading"></a></h2>
<p>Let’s now login to LUMI and start experimenting what we see and what’s available.</p>
<ol class="arabic simple">
<li><p>Log in to LUMI with your user credentials (SSH or <a class="reference external" href="https://www.lumi.csc.fi">LUMI web interface</a>):</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-i<span class="w"> </span>&lt;path-to-private-key&gt;<span class="w"> </span>&lt;username&gt;@lumi.csc.fi<span class="w">    </span><span class="c1"># replace &lt;username&gt; with your LUMI username, e.g. myname@lumi.csc.fi</span>
<span class="w">                                                       </span><span class="c1"># replace &lt;path-to-private-key&gt; with the path to your private key file, e.g. </span>
<span class="w">                                                       </span><span class="c1"># ./.ssh/id_ed25519 or /home/user/.ssh/id_ed25519</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Try a <code class="docutils literal notranslate"><span class="pre">module</span></code> command! Check out which modules are loaded as default as you login to LUMI:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>list
</pre></div>
</div>
<p>The modules you see listed are both <em>installed</em> to the system, and currently <em>loaded</em>, which means that they are ready for use. This is the default environment when you open a new session on LUMI, before you do, or load, anything else, and contains some initial/default settings for libraries, compiling environment, etc.</p>
<p>In your regular use of LUMI, the next step after login would usually be loading one version of the available software stacks. Which one, depends on what we would like to do on LUMI.</p>
<p>One example workflow would be that as a first after login we would load a version of LUMI software stacks (e.g. <code class="docutils literal notranslate"><span class="pre">LUMI/23.09</span></code>). Then we would load a suitable partition for our use case (e.g. <code class="docutils literal notranslate"><span class="pre">partition/C</span></code> that is suitable for the LUMI-C compute nodes). (The partitions are there to add some settings on top of the software stack, depending on which types of nodes we are using: login nodes, LUMI-C nodes, or LUMI-G nodes.) Then we would load our software. If the software is not yet installed (usually by you or by one of your group members to your <code class="docutils literal notranslate"><span class="pre">/project/project_46XXXXXXX/</span></code> location on the LUMI file system), we install the software first.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Login<span class="w"> </span><span class="o">(</span>Basic<span class="w"> </span>Cray<span class="w"> </span>environment<span class="w"> </span>loaded<span class="w"> </span>as<span class="w"> </span>default<span class="o">)</span>
<span class="p">|</span>
<span class="p">|</span>--------------<span class="w"> </span>CrayEnv
<span class="p">|</span><span class="w">               </span><span class="sb">`</span>-----<span class="w"> </span><span class="o">(</span>Load<span class="w"> </span>or<span class="w"> </span>install<span class="w"> </span>your<span class="w"> </span>environments/packages<span class="o">)</span>
<span class="p">|</span><span class="w">      </span>
<span class="p">|</span>--------------<span class="w"> </span>LUMI/22.12
<span class="p">|</span>
<span class="p">|</span>--------------<span class="w"> </span>LUMI/23.03
<span class="p">|</span><span class="w">           </span>
<span class="p">|</span>--------------<span class="w"> </span>LUMI/23.09
<span class="p">|</span><span class="w">                  </span><span class="p">|</span>
<span class="p">|</span><span class="w">                  </span><span class="p">|</span>----<span class="w"> </span>partition/L
<span class="p">|</span><span class="w">                  </span><span class="p">|</span>----<span class="w"> </span>partition/C
<span class="p">|</span><span class="w">                  </span><span class="sb">`</span>----<span class="w"> </span>partition/G
<span class="p">|</span><span class="w">                        </span><span class="sb">`</span>------<span class="w"> </span>Load<span class="w"> </span>or<span class="w"> </span>install<span class="w"> </span>your<span class="w"> </span>software<span class="w"> </span><span class="o">(</span>e.g.<span class="w"> </span>w/<span class="w"> </span>EasyBuild<span class="o">)</span><span class="w"> </span>
<span class="p">|</span>
<span class="p">|</span>--------------<span class="w"> </span>Spack/23.03
<span class="p">|</span>
<span class="sb">`</span>--------------<span class="w"> </span>Spack/23.09
<span class="w">                </span>
</pre></div>
</div>
<p>To see other modules that are currently installed and available to be loaded (but not yet loaded to use), one can use the command <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code>. The output lists everything that is currently available to be loaded, so it’s quite a long list. (Typing this opens an output with the <code class="docutils literal notranslate"><span class="pre">less</span></code> tool, which you can scroll e.g. with the keyboard arrow kyes, and exit with pressing <code class="docutils literal notranslate"><span class="pre">q</span></code> from the keyboard.)</p>
<p>To search if a specific software is directly availabe to load, one can use the command <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;software_name&gt;</span></code> . E.g. to see, if we have the simple text editor <code class="docutils literal notranslate"><span class="pre">Nano</span></code> available to load:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>spider<span class="w"> </span>nano
</pre></div>
</div>
<p>The output is an overview of versions of Nano that are available to be loaded. In general it’s good to use the latest versions of a software available, unless one has a reason to use an older version. Let’s see more information of some of the Nano versions:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>spider<span class="w"> </span>nano/7.2
</pre></div>
</div>
<p>We get an output that gives us some information about this software or the version of it, and what we need to do first to load this version of Nano, if anything.
We see that this version of Nano is available in the <code class="docutils literal notranslate"><span class="pre">CrayEnv</span></code> software stack, and also in several versions of the main <code class="docutils literal notranslate"><span class="pre">LUMI</span> <span class="pre">software</span> <span class="pre">stacks</span></code>.</p>
<p>What it comes to the Nano editor itself, it doesn’t really matter which version of software stacks and which <em>partition</em> we choose. This is relevant what it comes to other things we are doing on LUMI besides using the Nano editor (since probably you didn’t apply compute time on LUMI to use the Nano editor, but rather are using Nano in addition to some other workflow). In some cases it matters how a software is optimized or compiled, and all the software one is using in the same workflow should be optimized or compiled the way that it is compatible. This is the reason why there are so many different versions for such a simple tool as Nano available, for example.</p>
<p>From the output of available prerequisites that we just got with <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">nano/7.2</span></code> we choose <strong>one line</strong>. Let’s now choose the latest (on May 2024) of the available LUMI software stacks <code class="docutils literal notranslate"><span class="pre">LUMI/23.09</span></code>. Besides this let’s choose to use the <code class="docutils literal notranslate"><span class="pre">partition/L</span></code> that just means a set of settings optimized for the <em>login nodes</em> on LUMI. Because we are just going to test Nano on a login node where we are now, and not submitting jobs e.g. to LUMI-C or LUMI-G and using Nano interactively from a compute node for example, we are happy with settings optimized for the login nodes.</p>
<p>At this point we are still with the same environment than which we had right after login to LUMI. If we do the <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">list</span></code> the listing is exactly the same as earlier.</p>
<hr class="docutils" />
<p>Let’s see the currently available versions of <code class="docutils literal notranslate"><span class="pre">LUMI</span></code> software stacks:</p>
<p>Type <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">LUMI</span></code> and press the tabulator key twice. The output you see should be something like:</p>
<p><code class="docutils literal notranslate"><span class="pre">LUMI</span>&#160; <span class="pre">LUMI/22.08</span>&#160; <span class="pre">LUMI/22.12</span>&#160; <span class="pre">LUMI/23.03</span>&#160; <span class="pre">LUMI/23.09</span></code></p>
<p>The versions of the LUMI software stacks are named based on a year and a month that the corresponding version of a Cray software stack was reliesed.</p>
<p>Now let’s not load any version of the LUMI software stacks just yet. Usually it’s good to load a version of the software stacks after one knows which version we need, and we’ll get to that point soon.</p>
<p>When loading e.g. a LUMI software stak version, if you don’t specify the version in the module load command (i.e. you would do only <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">LUMI</span></code> and press enter), a default version is loaded. But it is recommended to also provide the version of the software stacks in the module load command, as we are doing above, as the default version may change. Also when specifying the version, you always know what you are actually loading, and what other modules or EasyBuild installations are compatible with it.</p>
</section>
<hr class="docutils" />
<section id="tutorial-load-the-nano-editor-and-write-a-simple-script">
<h2>Tutorial: Load the Nano editor and write a simple script<a class="headerlink" href="#tutorial-load-the-nano-editor-and-write-a-simple-script" title="Permalink to this heading"></a></h2>
<p>Work in progress</p>
<p>Maybe a simple example like this would be good here.
It could be some other than Nano, but should be something that is there that can be just directly loaded without the installation step.</p>
</section>
<section id="tutorial-install-a-software-with-easybuild">
<h2>Tutorial: Install a software with EasyBuild<a class="headerlink" href="#tutorial-install-a-software-with-easybuild" title="Permalink to this heading"></a></h2>
<p>Let’s use here an example software called ‘eb-tutorial’. This is an example software that has no real practical use, but can be used in exercises as an example of a software, when getting to know EasyBuild.
First let’s check the available versions of the software from the <a class="reference external" href="https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/">LUMI software library</a>.
At <code class="docutils literal notranslate"><span class="pre">e</span></code> we find the <a class="reference external" href="https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/e/eb-tutorial/">eb-tutorial</a>. We see from the blue box on top that it is <code class="docutils literal notranslate"><span class="pre">user-installable</span></code>, which means that we can install it with EasyBuild.</p>
<p>Let’s scroll down a bit on the page, to the list of available EasyConfigs. It is in general a good idea to use the latest available installation, unless there are other restrictions (e.g. one needs a certain version of a software, or because of compatibility reasons).
We see that there is available one version of this software <code class="docutils literal notranslate"><span class="pre">1.0.1</span></code>. It is available as six different toolchains (May 19th 2024). The version of the compiler (GNU, Cray, AOCC) could have a meaning for us, but because we don’t now care how the software is compiled, we choose as an example the one that is compiled with the GNU compiler (cpeGNU). Because we don’t have other restrictions, we choose the installation made to the latest LUMI sotware stack that is available, the 22.12.</p>
<p>(A picture here about the text <code class="docutils literal notranslate"><span class="pre">eb-tutorial-1.0.1-cpeGNU-22.12.eb</span></code> where the software name, version, and eb-toolchain are marked?)</p>
<p>=&gt; We will install the <code class="docutils literal notranslate"><span class="pre">eb-tutorial-1.0.1-cpeGNU-22.12.eb</span></code> version of the eb-tutorial software.</p>
<p>(Some boxes with different colours next? To separate the blocks that one needs to do only once, and the ones that needs to be done every time on LUMI?)</p>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Decide where you want to install the software. The default location is your home folder $HOME/EasyBuild. The software will be automatically installed there, if you don’t change anything. However, it’s in general a better practise to install the software in your /project folder, which is shared by everyone in your project. If the software is istalled there, all of the members of your project can use the installed software that one of the project members has installed.</p></li>
</ol>
<p>Now let’s change the location for EasyBuild installations to the /project folder of your project.</p>
<p>Note, that this change needs to be done before you load any version of the LUMI sotware stacks.</p>
<p>Open a new shell on LUMI (e.g. logout, and login to LUMI again, or open a new LUMI session in another shell window)</p>
<p>Go to your home folder:</p>
<p><code class="docutils literal notranslate"><span class="pre">cd</span></code></p>
<p>List the content of your home folder with a command that also shows the hidden files:</p>
<p><code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">-a</span></code></p>
<p>Open the <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> file e.g. with</p>
<p><code class="docutils literal notranslate"><span class="pre">nano</span> <span class="pre">.bashrc</span></code></p>
<p>Add a following line somewhere in the file</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">EBU_USER_PREFIX</span><span class="o">=</span>/project/project_465000000/EasyBuild<span class="w"> </span><span class="c1"># Replace the project number with your project number</span>
</pre></div>
</div>
<p>Save the changes to the <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> file with <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">s</span></code> and exit nano with <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">x</span></code> .</p>
<ol class="arabic simple" start="2">
<li><p>Next let’s load the version of LUMI software stack that corresponds to our software version:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">LUMI/22.12</span></code></p>
<ol class="arabic simple" start="3">
<li><p>Next we will decide which <code class="docutils literal notranslate"><span class="pre">partition</span></code> we will use the software with. If we don’t choose anything here, the default is <code class="docutils literal notranslate"><span class="pre">partition/L</span></code>, which means a set of settings that are optimized for the login nodes of LUMI. If we would be using this software on the compute nodes of LUMI-C, we would choose the <code class="docutils literal notranslate"><span class="pre">partition/C</span></code>. If the softare would be used on the LUMI-G nodes, we would load the <code class="docutils literal notranslate"><span class="pre">partition/G</span></code> . For the example let’s say that our software ‘eb-tutorial’ is something that we want to run on the LUMI-C compute nodes. In that case we install the software with settings that are optimized for the LUMI-C compute nodes:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">partition/C</span></code></p>
<ol class="arabic simple" start="4">
<li><p>To install a software with EasyBuild, we will need to load a module that does the installation:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">EasyBuild-user</span></code></p>
<p>This is loaded only when we are installing the software, not anymore later when we are using the software.</p>
<ol class="arabic simple" start="5">
<li><p>Now the software is installed with a command:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>eb<span class="w"> </span>eb-tutorial-1.0.1-cpeGNU-22.12.eb<span class="w"> </span>-r<span class="w"> </span><span class="c1">#The -r tells to also install possible dependencies</span>
</pre></div>
</div>
<p>Depending on the software, and if it needs to first install several dependencies, the installation can take a while.</p>
<ol class="arabic simple" start="6">
<li><p>Once the installation is ready, the software has appeared to us on LUMI as it would be installed in the central software stack. We can take it into use with:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">eb-tutorial</span></code></p>
</section>
<section id="usage-of-the-software">
<h3>Usage of the software<a class="headerlink" href="#usage-of-the-software" title="Permalink to this heading"></a></h3>
<p>When you later login to LUMI again and are using the software next times, you (or your group members) only need to do the steps:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12
module<span class="w"> </span>load<span class="w"> </span>partition/C
module<span class="w"> </span>load<span class="w"> </span>eb-tutorial
</pre></div>
</div>
</section>
</section>
<section id="tutorial-pytorch-with-ddp">
<h2>Tutorial: PyTorch with DDP<a class="headerlink" href="#tutorial-pytorch-with-ddp" title="Permalink to this heading"></a></h2>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Permalink to this heading"></a></h3>
<p>Run through the following three steps to initialize a PyTorch environment on LUMI. This only needs to be done once.</p>
<section id="st-step">
<h4>1st Step<a class="headerlink" href="#st-step" title="Permalink to this heading"></a></h4>
<p>Install ROCm Communication Collectives Library (RCCL) if you want to leverage multiple GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>LUMI/22.08<span class="w"> </span>partition/G
<span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>EasyBuild-user
<span class="gp">$ </span>eb<span class="w"> </span>aws-ofi-rccl-66b3b31-cpeGNU-22.08.eb<span class="w"> </span>-r
</pre></div>
</div>
</section>
<section id="nd-step">
<h4>2nd Step<a class="headerlink" href="#nd-step" title="Permalink to this heading"></a></h4>
<p>Get the ROCm PyTorch Docker container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">SINGULARITY_TMPDIR</span><span class="o">=</span>/tmp/tmp-singularity<span class="w"> </span><span class="nv">SINGULARITY_CACHEDIR</span><span class="o">=</span>/tmp/tmp-singularity<span class="w"> </span>singularity<span class="w"> </span>pull<span class="w"> </span>docker://rocm/pytorch:rocm5.7_ubuntu22.04_py3.10_pytorch_2.0.1
</pre></div>
</div>
</section>
<section id="rd-step">
<h4>3rd Step<a class="headerlink" href="#rd-step" title="Permalink to this heading"></a></h4>
<p>Create a virtual environment (venv) for each of your projects and add additional modules you might need:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>pytorch_rocm5.7_ubuntu22.04_py3.10_pytorch_2.0.1.sif<span class="w"> </span>bash
<span class="go">Singularity&gt; python -m venv my_project_env --system-site-packages</span>
<span class="go">Singularity&gt; . my_project_env/bin/activate</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; Install what else you&#39;d need: pip install ...</span>
</pre></div>
</div>
<div class="admonition-create-more-virtual-environments callout admonition" id="callout-0">
<p class="admonition-title">Create more virtual environments</p>
<p>You might want to create more virtual environments, one for each of your projects, or for experimenting with different module versions. Simply repeat the commands with providing individual virtual environment names.</p>
</div>
</section>
</section>
<section id="use-it">
<h3>Use it<a class="headerlink" href="#use-it" title="Permalink to this heading"></a></h3>
<p>We use the following job script (<code class="docutils literal notranslate"><span class="pre">run_sbatch.sh</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash -l
#SBATCH --job-name=&quot;PyTorch MNIST&quot;
#SBATCH --output=output.txt
#SBATCH --error=error.txt
#SBATCH --partition=standard-g
#SBATCH --nodes=2
#SBATCH --ntasks=16
#SBATCH --ntasks-per-node=8
#SBATCH --time=1:00:00
#SBATCH --account=project_&lt;YOURID&gt;
#SBATCH --gpus-per-node=8

module load LUMI/22.08 partition/G
module load singularity-bindings
module load aws-ofi-rccl

export SCRATCH=$PWD
export NCCL_SOCKET_IFNAME=hsn
export NCCL_NET_GDR_LEVEL=3
export MIOPEN_USER_DB_PATH=/tmp/${USER}-miopen-cache-${SLURM_JOB_ID}
export MIOPEN_CUSTOM_CACHE_DIR=${MIOPEN_USER_DB_PATH}
export CXI_FORK_SAFE=1
export CXI_FORK_SAFE_HP=1
export FI_CXI_DISABLE_CQ_HUGETLB=1
export SINGULARITYENV_LD_LIBRARY_PATH=${EBROOTAWSMINOFIMINRCCL}/lib:/opt/cray/xpmem/2.5.2-2.4_3.50__gd0f7936.shasta/lib64:${SINGULARITYENV_LD_LIBRARY_PATH}

master_addr=$(scontrol show hostnames &quot;$SLURM_JOB_NODELIST&quot; | head -n 1)
export SINGULARITYENV_MASTER_ADDR=&quot;$master_addr&quot;
export SINGULARITYENV_MASTER_PORT=6200
echo &quot;MASTER_ADDR=&quot;$SINGULARITYENV_MASTER_ADDR &quot;MASTER_PORT=&quot;$SINGULARITYENV_MASTER_PORT

srun --mpi=cray_shasta singularity exec --pwd /work --bind $SCRATCH:/work pytorch_rocm5.7_ubuntu22.04_py3.10_pytorch_2.0.1.sif /work/my_project_env/bin/python PyTorch_MNIST-DDP.py
</pre></div>
</div>
<p>This job script executes 16 processes over two GPU nodes. That is, one process for each of the eight GPUs in a single node. The first node is used as master node for NCCL/RCCL communication management with port 6200.</p>
<div class="admonition-using-the-project-scratch-volume callout admonition" id="callout-1">
<p class="admonition-title">Using the project scratch volume</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">project_&lt;YOURID&gt;</span></code> with your project ID. Consider to use the working directory <code class="docutils literal notranslate"><span class="pre">/scratch/project_&lt;YOURID&gt;</span></code> for running your experiemnts. Your virtual environment should also be located there.</p>
</div>
<p>Place the following <code class="docutils literal notranslate"><span class="pre">PyTorch_MNIST-DDP.py</span></code> script into your working directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel.distributed</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">dataset_loc</span> <span class="o">=</span> <span class="s1">&#39;./mnist_data&#39;</span>

<span class="c1">#rank = int(os.environ[&#39;OMPI_COMM_WORLD_RANK&#39;])</span>
<span class="c1">#local_rank = int(os.environ[&#39;OMPI_COMM_WORLD_LOCAL_RANK&#39;])</span>
<span class="c1">#world_size = int(os.environ[&#39;OMPI_COMM_WORLD_SIZE&#39;])</span>
<span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_PROCID&#39;</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_LOCALID&#39;</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_NTASKS&#39;</span><span class="p">])</span>

<span class="c1">#os.environ[&quot;MASTER_ADDR&quot;] = &quot;127.0.0.1&quot;</span>
<span class="c1">#os.environ[&quot;MASTER_PORT&quot;] = &quot;6108&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">],</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">])</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># convert and scale</span>
<span class="p">])</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">dataset_loc</span><span class="p">,</span>
                                   <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">dataset_loc</span><span class="p">,</span>
                                  <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">dataset_loc</span><span class="p">,</span>
                                   <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">dataset_loc</span><span class="p">,</span>
                                  <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                                   <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="c1"># number of all GPUs</span>
                                   <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>               <span class="c1"># (global) ID of GPU</span>
                                   <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">test_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span>
                                 <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="c1"># number of all GPUs</span>
                                 <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>               <span class="c1"># (global) ID of GPU</span>
                                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># sampler does it</span>
                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                          <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
                          <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span>   <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                          <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
                          <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>  <span class="c1"># Input: 28x28(x1) pixels</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Output: 10 classes</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>

<span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">local_rank</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Main training loop</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="c1">#train_loader.sampler.set_epoch(i)</span>

    <span class="c1"># Training steps per epoch</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">batch_loss_scalar</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">batch_loss_scalar</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;training batch_loss=</span><span class="si">{</span><span class="n">batch_loss_scalar</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># Run validation at the end of each epoch</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">batch_loss_scalar</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">batch_loss_scalar</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;validation batch_loss=</span><span class="si">{</span><span class="n">batch_loss_scalar</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, train_loss=</span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val_loss=</span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Execute the job script in the directory of your project which contains the virtual environment:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sbatch<span class="w"> </span>run_sbatch.sh
</pre></div>
</div>
<p>Refer to the <code class="docutils literal notranslate"><span class="pre">output.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">error.txt</span></code> files for the results.</p>
</section>
</section>
<section id="tutorial-tensorflow-with-horovod">
<h2>Tutorial: Tensorflow with Horovod<a class="headerlink" href="#tutorial-tensorflow-with-horovod" title="Permalink to this heading"></a></h2>
<section id="id1">
<h3>Setup<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<p>Run through the following three steps to initialize a Tensorflow environment on LUMI. This only needs to be done once.</p>
<section id="id2">
<h4>1st Step<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h4>
<p>Install ROCm Communication Collectives Library (RCCL) and OpenMPI if you want to leverage multiple GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>LUMI/22.08<span class="w"> </span>partition/G
<span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>EasyBuild-user
<span class="gp">$ </span>eb<span class="w"> </span>aws-ofi-rccl-66b3b31-cpeGNU-22.08.eb<span class="w"> </span>-r
<span class="gp">$ </span>eb<span class="w"> </span>OpenMPI-4.1.3-cpeGNU-22.08.eb<span class="w"> </span>-r
</pre></div>
</div>
</section>
<section id="id3">
<h4>2nd Step<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h4>
<p>Get the ROCm Tensorflow Docker container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>eb<span class="w"> </span>singularity-bindings-system-cpeGNU-22.08.eb<span class="w"> </span>-r
<span class="gp">$ </span><span class="nv">SINGULARITY_TMPDIR</span><span class="o">=</span>/tmp/tmp-singularity<span class="w"> </span><span class="nv">SINGULARITY_CACHEDIR</span><span class="o">=</span>/tmp/tmp-singularity<span class="w"> </span>singularity<span class="w"> </span>pull<span class="w"> </span>docker://rocm/tensorflow:rocm5.5-tf2.11-dev
</pre></div>
</div>
</section>
<section id="id4">
<h4>3rd Step<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h4>
<p>Install <code class="docutils literal notranslate"><span class="pre">ensurepip</span></code> first by executing the following script (<code class="docutils literal notranslate"><span class="pre">download_ensurepip.sh</span></code>)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>base_url=https://raw.githubusercontent.com/python/cpython/3.9/Lib
files=(&quot;__init__.py&quot; \
  &quot;__main__.py&quot; \
  &quot;_uninstall.py&quot; \
  &quot;_bundled/__init__.py&quot; \
  &quot;_bundled/pip-23.0.1-py3-none-any.whl&quot; \
  &quot;_bundled/setuptools-58.1.0-py3-none-any.whl&quot;)
for _f in ${files[@]}; do
  f=ensurepip/${_f}
  if test ! -f &quot;$f&quot;; then
    wget -q --show-progress ${base_url}/${f} -P $(dirname $f);
  else
    echo -e &quot;?${f}? already exists. Nothing to do.&quot;
  fi
done
</pre></div>
</div>
<p>with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>
<span class="gp">$ </span>bash<span class="w"> </span>download_ensurepip.sh
</pre></div>
</div>
<div class="admonition-adjust-for-your-versions callout admonition" id="callout-2">
<p class="admonition-title">Adjust for your versions</p>
<p>Depending on the Python environment you chose to use (as part of the Tensorflow Docker container), you might need to adjust the file names. E.g., the version of <code class="docutils literal notranslate"><span class="pre">pip</span></code> can be different. Please look at <code class="docutils literal notranslate"><span class="pre">https://github.com/python/cpython</span></code> in the <code class="docutils literal notranslate"><span class="pre">Lib/ensurepip</span></code> directory for the Python version (branch) used in the Docker conatiner.</p>
</div>
<p>Create a virtual environment (venv) for each of your projects and add additional modules you might need:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-B<span class="w"> </span><span class="nv">$HOME</span>/tensorflow/ensurepip:/usr/lib/python3.9/ensurepip<span class="w"> </span>tensorflow_rocm5.5-tf2.11-dev.sif<span class="w">  </span>bash
<span class="go">Singularity&gt; python -m venv my_project_env --system-site-packages</span>
<span class="go">Singularity&gt; . my_project_env/bin/activate</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; pip install tensorflow-datasets # Needed for later</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; Install what else you&#39;d need: pip install ...</span>
</pre></div>
</div>
<div class="admonition-create-more-virtual-environments callout admonition" id="callout-3">
<p class="admonition-title">Create more virtual environments</p>
<p>You might want to create more virtual environments, one for each of your projects, or for experimenting with different module versions. Simply repeat the commands with providing individual virtual environment names.</p>
</div>
</section>
<section id="th-step">
<h4>4th Step<a class="headerlink" href="#th-step" title="Permalink to this heading"></a></h4>
<p>Build and install Horovod. Use a clean environment for this!</p>
<div class="admonition-use-a-clean-environment callout admonition" id="callout-4">
<p class="admonition-title">Use a clean environment</p>
<p>Do not load <code class="docutils literal notranslate"><span class="pre">aws-ofi-rccl</span></code> or <code class="docutils literal notranslate"><span class="pre">singularity-bindings</span></code> modules before building Horovod as they interfere with the build process!</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>LUMI/22.08<span class="w"> </span>partition/G
<span class="gp">$ </span>module<span class="w"> </span>load<span class="w"> </span>EasyBuild-user
<span class="gp">$ </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>tensorflow_rocm5.5-tf2.11-dev.sif<span class="w">  </span>bash
<span class="go">Singularity&gt; . ./my_project_env/bin/activate</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_WITHOUT_MXNET=1</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_WITHOUT_PYTORCH=1</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_WITH_TENSORFLOW=1</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_GPU=ROCM</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_GPU_OPERATIONS=NCCL</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_ROCM_PATH=/opt/rocm</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_RCCL_HOME=/opt/rocm/rccl</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export RCCL_INCLUDE_DIRS=/opt/rocm/rccl/include</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HOROVOD_RCCL_LIB=/opt/rocm/rccl/lib</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HCC_AMDGPU_TARGET=gfx90a</span>
<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">Singularity&gt; export HIP_PATH=/opt/rocm</span>

<span class="gp gp-VirtualEnv">(my_project_env)</span> <span class="go">pip install --no-cache-dir --force-reinstall horovod==0.28.1</span>
</pre></div>
</div>
</section>
</section>
<section id="id5">
<h3>Use it<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h3>
<p>We use the following job script (<code class="docutils literal notranslate"><span class="pre">run_sbatch.sh</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash -l
#SBATCH --job-name=&quot;PyTorch MNIST&quot;
#SBATCH --output=output.txt
#SBATCH --error=error.txt
#SBATCH --partition=standard-g
#SBATCH --nodes=2
#SBATCH --ntasks=16
#SBATCH --ntasks-per-node=8
#SBATCH --time=1:00:00
#SBATCH --account=project_&lt;YOURID&gt;
#SBATCH --gpus-per-node=8

module load LUMI/22.08 partition/G
module load singularity-bindings
module load aws-ofi-rccl
module load OpenMPI/4.1.3-cpeGNU-22.08

export SCRATCH=$PWD
export NCCL_SOCKET_IFNAME=hsn
export MIOPEN_USER_DB_PATH=/tmp/${USER}-miopen-cache-${SLURM_JOB_ID}
export MIOPEN_CUSTOM_CACHE_DIR=${MIOPEN_USER_DB_PATH}
export CXI_FORK_SAFE=1
export CXI_FORK_SAFE_HP=1
export FI_CXI_DISABLE_CQ_HUGETLB=1
export SINGULARITYENV_LD_LIBRARY_PATH=/openmpi/lib:/opt/rocm/lib:${EBROOTAWSMINOFIMINRCCL}/lib:/opt/cray/xpmem/2.5.2-2.4_3.50__gd0f7936.shasta/lib64:$SINGULARITYENV_LD_LIBRARY_PATH

scontrol show hostname $SLURM_JOB_NODELIST &gt; hostfile
mpirun -np 16 -hostfile hostfile singularity exec --pwd /work --bind $SCRATCH:/work tensorflow_rocm5.5-tf2.11-dev.sif /work/my_project_env/bin/python keras_horovod_example.py
</pre></div>
</div>
<p>This job script executes 16 processes over two GPU nodes. That is, one process for each of the eight GPUs in a single node. This uses MPI for management/communication across the processes/nodes.</p>
<div class="admonition-using-the-project-scratch-volume callout admonition" id="callout-5">
<p class="admonition-title">Using the project scratch volume</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">project_&lt;YOURID&gt;</span></code> with your project ID. Consider to use the working directory <code class="docutils literal notranslate"><span class="pre">/scratch/project_&lt;YOURID&gt;</span></code> for running your experiemnts. Your virtual environment should also be located there.</p>
</div>
<p>Place the following <code class="docutils literal notranslate"><span class="pre">keras_horovod_example.py</span></code> script into your working directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Initialize Horovod</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="c1"># HVD</span>

<span class="n">physical_devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">physical_devices</span><span class="p">)</span>

<span class="n">tfds</span><span class="o">.</span><span class="n">disable_progress_bar</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">enable_v2_behavior</span><span class="p">()</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num GPUs available: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">))</span>
<span class="n">local_gpu</span> <span class="o">=</span>  <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span> <span class="c1">#HVD</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">local_gpu</span><span class="p">],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">urllib3</span>
<span class="n">urllib3</span><span class="o">.</span><span class="n">disable_warnings</span><span class="p">(</span><span class="n">urllib3</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">InsecureRequestWarning</span><span class="p">)</span>

<span class="p">(</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">ds_test</span><span class="p">),</span> <span class="n">ds_info</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;mnist&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span>
    <span class="n">shuffle_files</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ds_train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">assert_cardinality</span><span class="p">(</span><span class="n">ds_info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span><span class="p">))</span>
<span class="n">ds_test</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">assert_cardinality</span><span class="p">(</span><span class="n">ds_info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span><span class="p">))</span>

<span class="n">num_steps_train</span> <span class="o">=</span> <span class="n">ds_info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span> <span class="o">//</span> <span class="n">batch_size</span>
<span class="n">num_steps_test</span> <span class="o">=</span> <span class="n">ds_info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span> <span class="o">//</span> <span class="n">batch_size</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total number of training steps for training/testing: </span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_steps_train</span><span class="p">,</span> <span class="n">num_steps_test</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">normalize_img</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Normalizes images: `uint8` -&gt; `float32`.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">,</span> <span class="n">label</span>

<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">normalize_img</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">ds_info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span> <span class="c1"># HVD</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>

<span class="n">ds_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">normalize_img</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">ds_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">ds_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span> <span class="c1"># HVD</span>
<span class="n">ds_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">ds_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">inp</span><span class="p">)</span>
    <span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">flat</span><span class="p">)</span>
    <span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">dense1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">dense2</span><span class="p">)</span>

<span class="n">fmodel</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Horovod: broadcast initial variable states from rank 0 to all other processes.</span>
    <span class="c1"># This is necessary to ensure consistent initialization of all workers when</span>
    <span class="c1"># training is started with random weights or restored from a checkpoint.</span>
    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># HVD</span>
<span class="p">]</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.001</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># HVD</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span> <span class="c1"># HVD</span>

<span class="c1"># Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
<span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
<span class="n">fmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
    <span class="n">experimental_run_tf_function</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># HVD</span>
<span class="p">)</span>

<span class="c1"># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.</span>
<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># HVD</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;./checkpoint-</span><span class="si">{epoch}</span><span class="s1">.h5&#39;</span><span class="p">))</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of processes: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

<span class="n">time_s</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">fmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">ds_train</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">num_steps_train</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="c1"># HVD</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">ds_test</span><span class="p">,</span>
    <span class="n">validation_steps</span><span class="o">=</span><span class="n">num_steps_test</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="c1"># HVD</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Runtime: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_s</span><span class="p">))</span>
</pre></div>
</div>
<p>Execute the job script in the directory of your project which contains the virtual environment:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sbatch<span class="w"> </span>run_sbatch.sh
</pre></div>
</div>
<p>Refer to the <code class="docutils literal notranslate"><span class="pre">output.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">error.txt</span></code> files for the results.</p>
</section>
</section>
<section id="tutorial-adding-your-own-easyconfig-to-lumi">
<h2>Tutorial: Adding your own EasyConfig to LUMI<a class="headerlink" href="#tutorial-adding-your-own-easyconfig-to-lumi" title="Permalink to this heading"></a></h2>
<p>If you have an EasyConfig that you have written yourself, or modified from an existing one, and you would like to bring it to LUMI, here’s an example of how it’s done. This is briefly described in the <a class="reference external" href="https://docs.lumi-supercomputer.eu/software/installing/easybuild/#building-your-own-easybuild-repository">LUMI documentation</a></p>
<p>First let’s check what is the value of <code class="docutils literal notranslate"><span class="pre">EBU_USER_PREFIX</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span><span class="nv">$EBU_USER_PREFIX</span>
</pre></div>
</div>
<p>The output should be something as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/project/project_465000001/EasyBuild
</pre></div>
</div>
<p>or</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">$HOME</span>/EasyBuild
</pre></div>
</div>
<p>if we haven’t set the <code class="docutils literal notranslate"><span class="pre">EBU_USER_PREFIX</span></code> to point to a location under our project directories.</p>
<p>The location where <code class="docutils literal notranslate"><span class="pre">EBU_USER_PREFIX</span></code> points is not only where EasyBuild places the EasyBuild installations, but where it goes looking for existing recipes. If we want to add an EasyConfig ourselves, this is where to place it.</p>
<p>Let’s assume we have our <code class="docutils literal notranslate"><span class="pre">EBU_USER_PREFIX</span></code> variable pointing to <code class="docutils literal notranslate"><span class="pre">/project/project_465000001/EasyBuild</span></code>. Let’s go to this location</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/project/project_465000001/EasyBuild
</pre></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">ls</span></code> we see that the location contains something as the following, depening a bit what kind of installations one has already made:</p>
<p><code class="docutils literal notranslate"><span class="pre">ebrepo_files</span>&#160; <span class="pre">mgmt</span>&#160; <span class="pre">modules</span>&#160; <span class="pre">sources</span>&#160; <span class="pre">SW</span>&#160; <span class="pre">UserRepo</span></code></p>
<p>Let’s move to the location where the new recipe should be placed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>UserRepo/easybuild/easyconfigs
</pre></div>
</div>
<p>This location should be empty, if you haven’t added any EasyConfigs yourself before.</p>
<p>You should name your EasyConfig using a similar naming scheme as described in the LUMI documentation:
https://docs.lumi-supercomputer.eu/software/installing/easybuild/#easybuild-recipes</p>
<p>with</p>
<p><code class="docutils literal notranslate"><span class="pre">software_name</span></code>-<code class="docutils literal notranslate"><span class="pre">version</span></code>-<code class="docutils literal notranslate"><span class="pre">compiler</span></code>-<code class="docutils literal notranslate"><span class="pre">LUMI_software_stack_version</span></code>-<code class="docutils literal notranslate"><span class="pre">possible_additional_information</span></code>.eb</p>
<p>Create a new file in this location e.g. with using <code class="docutils literal notranslate"><span class="pre">nano</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nano<span class="w"> </span>&lt;your-new-easyconfig-name&gt;.eb
</pre></div>
</div>
<p>and add the content of the recipe there. Save and exit the document with <code class="docutils literal notranslate"><span class="pre">Ctrl+x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and Enter.</p>
<p>We can check with <code class="docutils literal notranslate"><span class="pre">ls</span></code> that our EasyConfig file is there now.</p>
<p>It doesn’t matter in which location you are next for the installation, but you can move e.g. to your home directory</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span>
</pre></div>
</div>
<p>To make sure you have a clean shell environment, it is recommended e.g. to log out from LUMI and back in again at this point.</p>
<p>When back at LUMI with a clean shell, we can now install the software module (for us and our project members) with using the EasyConfig we just added. This works the same way as with any other EasyConfigs that we have described earlier on this page, in the previous examples.</p>
<p>Let’s assume our EasyConfig is for LUMI/23.09, and we would like to use it with LUMI-C.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09
module<span class="w"> </span>load<span class="w"> </span>partition/C
module<span class="w"> </span>load<span class="w"> </span>EasyBuild-user
</pre></div>
</div>
<p>now, if we want to, we can check with <code class="docutils literal notranslate"><span class="pre">eb</span> <span class="pre">-S</span> <span class="pre">&lt;our-software-name&gt;</span></code> what recipes are available. If our software would be called e.g. MySoftware, we could do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>eb<span class="w"> </span>-S<span class="w"> </span>MySoftware
</pre></div>
</div>
<p>Let’s now install our EasyConfig:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>eb<span class="w"> </span>&lt;full-name-of-our-easyconfig&gt;.eb<span class="w"> </span>-r
</pre></div>
</div>
<p>This may take a while, depending on the software.</p>
<p>Now with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>avail<span class="w"> </span>&lt;our-software-name&gt;<span class="w"> </span>
</pre></div>
</div>
<p>we should find out that our software is available to be loaded with LUMI/23.09 and with partition/C, and we can load it with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>MySoftware
</pre></div>
</div>
<p>In the future, when logging in to LUMI, we can only load the same LUMI software stack and partition, and then our software. In this case it would be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09
module<span class="w"> </span>load<span class="w"> </span>partition/C
module<span class="w"> </span>load<span class="w"> </span>MySoftware
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../14-modules-and-software-stacks/" class="btn btn-neutral float-left" title="Software on supercomputers - Introduction to the concept of modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../16-scheduler-and-batch-jobs/" class="btn btn-neutral float-right" title="Scheduler and batch jobs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>